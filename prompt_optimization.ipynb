{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization\n",
    "\n",
    "This notebook demonstrates methods/intuitions for prompt optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Optimization Approaches\n",
    "\n",
    "### 1. Bayesian Optimization (ie. DsPy)\n",
    "Proposal Generation:\n",
    "- Uses LLM to generate (input, LM_output) pairs given (input, output) dataset\n",
    "- Uses program structure, dataset summaries, and Bayesian Optimization to generate proposals.\n",
    "\n",
    "Proposal Selection:\n",
    "- Uses probabilistic Bayesian model to rank proposals.\n",
    "\n",
    "Proposal Testing:\n",
    "- Evaluates prompts on subsets of data.\n",
    "\n",
    "### 2. Evolutionary Algorithms (ie. EvoPrompt)\n",
    "Proposal Generation:\n",
    "- Given population, uses LLM-based mutation/crossover operators to generate proposals.\n",
    "\n",
    "Proposal Selection:\n",
    "- Samples subset of prompts, evaluates them on \"development set\" (ie. subset of data), and picks top performers based on \"fitness\" ie. your given metrics.\n",
    "\n",
    "Proposal Testing:\n",
    "- Runs proposal selection and then returns best-performing prompt.\n",
    "\n",
    "### 3.\"Gradient\"-based Methods (ie. ProTeGI, APE)\n",
    "Proposal Generation:\n",
    " - Uses LLM, textual feedback, and previous prompt to generate future proposals.\n",
    "\n",
    "Proposal Selection:\n",
    " - Samples subset of prompts, evaluates them on subset of data, and uses evaluations to update prompt manually or to pick future prompt.\n",
    "\n",
    "Proposal Testing:\n",
    "- Samples subset of data and evaluates all prompts on subset based on metric, returning best-performing one. \n",
    "\n",
    "### 4. Human-interactive Methods (ie. iPrOp)\n",
    "Proposal Generation:\n",
    "- Uses LLM to generate variations and present to human in multi-armed bandit interface to learn reward for generating new proposals.\n",
    "\n",
    "Proposal Selection:\n",
    "- Uses human-selected feedback to filter proposals.\n",
    "\n",
    "Proposal Testing:\n",
    "- Evaluate filtered prompts on subset of data on given metric.\n",
    "\n",
    "### 5. Translation-based Methods (ie. BPO)\n",
    "Proposal Generation:\n",
    "- Uses LLM to generate critiques to generate \"optimal\" (intput, LM_output) pairs.\n",
    "- Train a seq2seq model to decode \"optimal\" prompts.\n",
    "\n",
    "Proposal Selection:\n",
    "- Can view (input, optimal LM_output) as means of selection.\n",
    "\n",
    "Proposal Testing:\n",
    "- Evaluate generated prompt to baseline and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveOptimizationLoop:\n",
    "    def init(self, optimizers, validators, generator):\n",
    "        self.optimizers = optimizers\n",
    "        self.validators = validators\n",
    "        self.generator = generator \n",
    "\n",
    "        self.history = []\n",
    "        self.data = []\n",
    "\n",
    "        self.n_iter = 1000\n",
    "\n",
    "\n",
    "def generate_proposals(self, feedback):\n",
    "    if feedback:\n",
    "        return self.generator(self.data, feedback)\n",
    "    else:\n",
    "        return self.generator(self.data)\n",
    "\n",
    "\n",
    "def run_iteration(self, current_prompt):\n",
    "    # 1. Evaluation\n",
    "    eval_results = self.evaluate_prompt(current_prompt)\n",
    "\n",
    "    # 2. Error Analysis\n",
    "    error_patterns = self.analyze_errors(eval_results)\n",
    "    \n",
    "    # 3. Optimization\n",
    "    proposals = self.generate_proposals(error_patterns)\n",
    "    \n",
    "    # 4. Validation\n",
    "    best_proposal = self.validate_proposals(proposals)\n",
    "    \n",
    "    return best_proposal\n",
    "\n",
    "\n",
    "def main(self):\n",
    "    # 1. Initialization\n",
    "    proposals = self.generate_proposals(feedback=None)\n",
    "    best_proposal = proposals[0]\n",
    "\n",
    "    # 2. Iteration\n",
    "    for iter in range(self.n_iter):\n",
    "        best_proposal = self.run_iteration(proposals)\n",
    "\n",
    "    return best_proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given (input, output) pairs and in some cases, an initial prompt, prompt optimization does the following:\n",
    "\n",
    "    (1) Use LLM to generate prompt candidates\n",
    "\n",
    "    Iterate over (2):\n",
    "\n",
    "    (2) Use an LLM to filter/critique the prompt candidates (ie. update) and/or generate additional (input, output) pairs\n",
    "\n",
    "    (3) Use an LLM to evaluate output of iteration loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-offs\n",
    "\n",
    "### 1. Bayesian Optimization (ie. DsPy)\n",
    "\n",
    "Best For: Multi-stage pipelines with multiple chained prompts (ie. RAG, code generation)\n",
    "\n",
    "Limitations: Overkill for simple tasks; requires program definitions in DsPY which can be unnecessary \n",
    "\n",
    "### 2. Evolutionary Algorithms (ie. EvoPrompt)\n",
    "Best For: Open-ended creativity and exploring new solutions (ie. story generation, brainstorming)\n",
    "\n",
    "Limitations: High computational cost, not compatible with multi-stage problems\n",
    "\n",
    "### 3.\"Gradient\"-based Methods (ie. ProTeGI, APE)\n",
    "Best For: General NLP tasks with clear input-output pairs (ie. translation) that can be optimized via incremental refinement (ie. stylistic alignment)\n",
    "\n",
    "Limitations: Not good for open-ended, creative tasks, generally low exploration\n",
    "\n",
    "\n",
    "\n",
    "### 4. Human-interactive Methods (ie. iPrOp)\n",
    "Best For: Domain-specific tuning (ie. medical/legal fields)\n",
    "\n",
    "Limitations: Limited scalability due to need of experts, systems may also be fickle with convergence depending on expert feedback consistency\n",
    "\n",
    "### 5. Translation-based Methods (ie. BPO)\n",
    "Best For: Human-preferences alignment (e.g., chatbot tuning, content moderation)\n",
    "\n",
    "Limitations: Requires large datasets usually; objectives are inflexible and require multiple models for multi-objective problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems for Prompt Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Optimization Trajectories: \n",
    "- Apply different optimization methods (1-5), showing their convergence patterns over iterations. iPrOp requires human supervision, and others are automatic.\n",
    "- Explore ensembling of prompts based on proposal selection/filtering during the optimization process.\n",
    "\n",
    "### 2. Prompt Inference:\n",
    "- Map top prompts to embedding space for visualization of similarity over performance.\n",
    "- Run inference using top prompts on held-out dataset. \n",
    "\n",
    "### 3. Multi-Objective Evaluation: \n",
    "- Use parallel coordinates plot to visualize trade-offs between different quantiative evaluation metrics (ie. accuracy, latency).\n",
    "\n",
    "### 4. LM + Human Evaluation: \n",
    "- Visualize the distribution of different types of errors identified by LLM judges based on the simulations (ie. sunburst chart).\n",
    "\n",
    "### 5. Performance Drift:\n",
    "- Repeat 2-4 regularly to monitor performance over time, starting at 1 if failure.\n",
    "- Interactive plot showing performance drift over time with daily variations and trend line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future directions/approaches\n",
    "- Investigate the use of prompt ensembling: (1) merging prompts into one and (2) combining the answers from each prompt method.\n",
    "\n",
    "- Design a meta-learning system where a learned subnetwork identifies which optimization method will yield the optimal prompt under different conditions.\n",
    "\n",
    "- Understand the model dependencies (ie. architecture, model size) of prompt optimization methods and their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
